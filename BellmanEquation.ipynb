{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake: Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, an agent is acting in an environment, and learning by trial-and-error to optimize its performance in order to gain maximal cumulative reward. The model of the environment can be formalized by a Markov Decision Process (MDP). To solve the MDP, value functions are defined, which give the expected cumulative reward in a given state. These value functions can be expressed mathematically using the Bellman equations. Solving the MDP comes down to solving these recursive equations. Because of the large number of equations, solving this system is done iteratively applying dynamic programming (DP). In this notebook the Bellman equation applied in the Value Iteration algorithm is discussed and implemented using numpy. Different implementations using loops, list comprehensions, and numpy array operations are tested and compared to see which of these implementations is computationally most efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different Bellman equations and the one tested in this notebook is the so called Bellman expectation equation for the state-value function $ V $:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ V _k(s) = \\sum \\limits _{a} \\left( \\pi(a|s) \\sum \\limits _{s'} P(s,a,s') \\left[ R(s,a,s') +\\gamma V _{k-1}(s') \\right] \\right) $\n",
    "\n",
    "with $ s $ the state, $ a $ the action, $ s' $ the next state, $ \\pi $ the policy, $ P $ the state transition probability, $ R $ the reward, $ \\gamma $ the discount factor, and $ k $ the iteration number. The equation is solved iteratively, starting with $ V _0(s) = 0 $ for all $ s $. The iterative process stops when a given number of iterations is reached or when a given criterion of convergence is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above equation is applied in the Policy Evaluation algorithm. Both Value Iteration planning and learning algorithms apply the Bellman optimality equation for the state-value function $ V $, which is the same equation as above, except that the first summation $ \\sum \\limits _{a} $ is replaced by the maximum function $ \\max \\limits _{a} $. In case of the Value Iteration planning algorithm, there is also no policy $ \\pi(a|s) $. Policy Improvement uses a similar equation to determine the action-value function $ Q(s,a) $. In this case the argmax function is used with random tie breaking instead of the summation or max function, and there is no policy $ \\pi(a|s) $ either. \n",
    "\n",
    "Because all algorithms that determine the state-value function apply a Bellman equation similar to the above equation, testing the latter suffies to optimize all of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorganizing the equation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest implementation of the iterative process solving the above Bellman equation uses four loops: an iteration loop, a loop for the states $ s $, a loop for the actions $ a $, and a loop for the next states $ s' $. That is the first implementation: loops only. A second implementation replaces the last two loops by a two nested list comprehensions and makes use of Python's built-in sum function.\n",
    "\n",
    "In numpy, however, it is not computationallly efficient to use loops, and therefore, it is recommended to apply vectorized expressions and array operations as much as possible. For this reason, the tested Bellman equation is reformulated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ V _k(s) = \\sum \\limits _{a} \\left( \\pi(a|s) \\left[ \\sum \\limits _{s'} P(s,a,s')R(s,a,s') + \\sum \\limits _{s'} \\gamma P(s,a,s') V _{k-1}(s') \\right] \\right) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first implementation using numpy array operations keeps the loop going through all states $ s $, but it replaces the two inner loops for the actions $ a $ and the next states $ s' $: \n",
    "- the summation $ \\sum \\limits _{s'} P(s,a,s')R(s,a,s') $ can be performed using the element-wise matrix multiplication (or Hadamard product) and numpy's sum function for arrays; \n",
    "- the summation $ \\sum \\limits _{s'} \\gamma P(s,a,s')V _{k-1}(s') $ can be done applying numpy's dot function for matrix multiplication;\n",
    "- to execute the outer summation over all actions $ a $, numpy's sum function for arrays can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last implementation using numpy array operations even avoids looping through all states $ s $ by reshaping the 3D arrays $ P(s,a,s') $ and $ R(s,a,s') $. Both arrays have size $ n_s $ x $ n_a $ x $ n_s $, with $ n_s $ the number of states and $ n_a $ the number of actions. Reshaping them to matrices of size $ (n_s.n_a) $ x $ n_s $, and performing the inner summutions over all next states $ s' $ for all actions of all states instead of all actions of one state, replaces the outer state $ s $ loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four implemenations are tested in this notebook to verify if they all give the same results. A large number of iterations is performed and time is recorded for each implemenation to see which one is computationally the most efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import module ReinforcementLearning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic Frozen Lake MDP is used as test case. Therefore the \"ReinforcementLearning\" module is imported so the required matrices are obtained easily. Also the package \"timeit\" is imported from which function \"time\" will be used to record time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReinforcementLearning import *\n",
    "from timeit import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stochastic \"FrozenLake\" environment is created and its MDP is constructed using class \"GymMDP\". The uniform random policy is defined using class \"UniformRandomPolicy\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLake.make()\n",
    "mdp = GymMDP(env)\n",
    "policy = UniformRandomPolicy(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally some useful variables are assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = env.nstates  # number of states\n",
    "na = env.nactions  # number of actions\n",
    "niter = 10000  # number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first implementation uses three loops to solve the Bellman equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:\n",
      "14.875230073928833\n",
      "\n",
      "Value function:\n",
      "[0.0139398  0.01163093 0.02095299 0.01047649 0.01624867 0.\n",
      " 0.04075154 0.         0.0348062  0.08816993 0.14205316 0.\n",
      " 0.         0.17582037 0.43929118 0.        ]\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "Vs = np.zeros(ns)\n",
    "\n",
    "for i in range(niter):\n",
    "    for s in range(ns):\n",
    "        Qsa = []\n",
    "        for a in range(na):\n",
    "            qsum = 0.0\n",
    "            for n in range(ns):\n",
    "                 qsum += mdp.Psas[s, a, n] * (mdp.Rsas[s, a, n] + mdp.gamma * Vs[n])\n",
    "            Qsa.append(policy.prob[s, a] * qsum)\n",
    "        Vs[s] = sum(Qsa)  # replace sum by max for value iteration learning\n",
    "\n",
    "time2 = time.time()\n",
    "\n",
    "print(\"Elapsed time:\")\n",
    "print(time2-time1)\n",
    "print(\"\\nValue function:\")\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer loop and nested list comprehensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second implementation, the two inner loops are replaced by nested list comprehension and function sum is applied on the resulting list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:\n",
      "15.317014455795288\n",
      "\n",
      "Value function:\n",
      "[0.0139398  0.01163093 0.02095299 0.01047649 0.01624867 0.\n",
      " 0.04075154 0.         0.0348062  0.08816993 0.14205316 0.\n",
      " 0.         0.17582037 0.43929118 0.        ]\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "Vs = np.zeros(ns)\n",
    "\n",
    "for i in range(niter):\n",
    "    for s in range(ns):\n",
    "        Vs[s] = sum([policy.prob[s, a] *  # replace sum by max for value iteration learning\n",
    "                     sum([mdp.Psas[s, a, n] *\n",
    "                          (mdp.Rsas[s, a, n] + mdp.gamma * Vs[n])\n",
    "                          for n in range(ns)])\n",
    "                     for a in range(na)])\n",
    "\n",
    "time2 = time.time()\n",
    "\n",
    "print(\"Elapsed time:\")\n",
    "print(time2-time1)\n",
    "print(\"\\nValue function:\")\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer loop and array operations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third implementation replaces the two inner loops by numpy array operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:\n",
      "1.7662701606750488\n",
      "\n",
      "Value function:\n",
      "[0.0139398  0.01163093 0.02095299 0.01047649 0.01624867 0.\n",
      " 0.04075154 0.         0.0348062  0.08816993 0.14205316 0.\n",
      " 0.         0.17582037 0.43929118 0.        ]\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "Vs = np.zeros(ns)\n",
    "PR = np.sum(mdp.Psas * mdp.Rsas, axis=2)\n",
    "gPsas = mdp.gamma * mdp.Psas\n",
    "\n",
    "for i in range(niter):\n",
    "    for s in range(ns):\n",
    "        Vs[s] = np.sum(policy.prob[s, :] *  # replace np.sum by np.max for value iteration learning\n",
    "                       (PR[s, :] + np.squeeze(np.dot(gPsas[s, :, :], Vs))))\n",
    "\n",
    "time2 = time.time()\n",
    "\n",
    "print(\"Elapsed time:\")\n",
    "print(time2-time1)\n",
    "print(\"\\nValue function:\")\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array operations only "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last implementation replaces the three inner loops by reshaping the numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:\n",
      "0.13064908981323242\n",
      "\n",
      "Value function:\n",
      "[0.0139398  0.01163093 0.02095299 0.01047649 0.01624867 0.\n",
      " 0.04075154 0.         0.0348062  0.08816993 0.14205316 0.\n",
      " 0.         0.17582037 0.43929118 0.        ]\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "nsa = ns * na\n",
    "shape = (ns, na)\n",
    "prob = np.reshape(policy.prob, (nsa,), order=\"c\")\n",
    "PR = np.reshape(np.sum(mdp.Psas * mdp.Rsas, axis=2),\n",
    "                (nsa, ), order=\"c\")\n",
    "gPsa = mdp.gamma * np.reshape(mdp.Psas, (nsa, ns), order=\"c\")\n",
    "Vs = np.zeros(ns)\n",
    "\n",
    "for i in range(niter):\n",
    "    Qsa = np.reshape(prob * (PR + np.dot(gPsa, Vs)),\n",
    "                     shape, order=\"c\")\n",
    "    Vs = np.sum(Qsa, axis=1)  # replace np.sum by np.max for value iteration learning\n",
    "\n",
    "time2 = time.time()\n",
    "\n",
    "print(\"Elapsed time:\")\n",
    "print(time2-time1)\n",
    "print(\"\\nValue function:\")\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that using numpy array operations is a lot faster than using loops or list comprehensions. Considering the two implementations applying numpy array operations, the implementation that uses no loops at all to calucate the Bellman equation is still significantly more efficient than the implemenation that keeps the outer state $ s $ loop. So it is recommended to make use of this last implementation of the Bellman equations in all planning and learning algorithms. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
