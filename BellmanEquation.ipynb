{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake: Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, an agent is acting in an environment, and learning by trial-and-error to optimize its performance in order to gain maximal cumulative reward. The model of the environment can be formalized by a Markov Decision Process (MDP). To solve the MDP, value functions are defined, which give the expected cumulative reward in a given state. These value functions can be expressed mathematically using the Bellman equations, and solving the MDP comes down to solving these recursive equations. Because of the large number of equations, solving this system is done iteratively applying dynamic programming (DP). In this notebook the Bellman equation applied in the Value Iteration algorithm is discussed and implemented using numpy. Different implementations using loops, list comprehensions, and numpy array operations are tested and compared to see which of these implementations is computationally most efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ V _k(s) = \\max _{a}\\left( \\pi(a|s) \\sum \\limits _{s'} P _{sas'}[R _{sas'}+\\gamma V _{k-1}(s')] \\right) $\n",
    "\n",
    "\n",
    "$ V _k(s) = \\max _{a}\\left( \\pi(a|s) \\left[ \\sum \\limits _{s'} P _{sas'}R _{sas'} +\\sum \\limits _{s'} \\gamma P _{sas'} V _{k-1}(s') \\right] \\right) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReinforcementLearning import *\n",
    "from timeit import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLake.make()\n",
    "mdp = GymMDP(env)\n",
    "policy = UniformRandomPolicy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstates = env.nstates\n",
    "nactions = env.nactions\n",
    "niter = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.728909969329834\n",
      "[1.55787288e-06 5.61039750e-06 6.01564996e-05 6.01564996e-06\n",
      " 1.15262041e-05 0.00000000e+00 6.55705846e-04 0.00000000e+00\n",
      " 1.25230373e-04 1.36600790e-03 7.80831365e-03 0.00000000e+00\n",
      " 0.00000000e+00 8.45855072e-03 9.16780501e-02 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "Vs = np.zeros(nstates)\n",
    "\n",
    "for i in range(niter):\n",
    "    for s in range(nstates):\n",
    "        Qsa = []\n",
    "        for a in range(nactions):\n",
    "            qsum = 0.0\n",
    "            for n in range(nstates):\n",
    "                 qsum += mdp.Psas[s, a, n] * (mdp.Rsas[s, a, n] + mdp.gamma * Vs[n])\n",
    "            Qsa.append(policy.prob[s, a] * qsum)\n",
    "        Vs[s] = max(Qsa)\n",
    "\n",
    "time2 = time.time()\n",
    "print(time2-time1)\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer loop and list comprehensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.292079448699951\n",
      "[1.55787288e-06 5.61039750e-06 6.01564996e-05 6.01564996e-06\n",
      " 1.15262041e-05 0.00000000e+00 6.55705846e-04 0.00000000e+00\n",
      " 1.25230373e-04 1.36600790e-03 7.80831365e-03 0.00000000e+00\n",
      " 0.00000000e+00 8.45855072e-03 9.16780501e-02 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "Vs = np.zeros(nstates)\n",
    "\n",
    "for i in range(niter):\n",
    "    for s in range(nstates):\n",
    "        Vs[s] = max([policy.prob[s, a] *\n",
    "                     sum([mdp.Psas[s, a, n] *\n",
    "                          (mdp.Rsas[s, a, n] + mdp.gamma * Vs[n])\n",
    "                          for n in range(nstates)])\n",
    "                     for a in range(nactions)])\n",
    "\n",
    "time2 = time.time()\n",
    "print(time2-time1)\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer loop and array operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7982220649719238\n",
      "[1.55787288e-06 5.61039750e-06 6.01564996e-05 6.01564996e-06\n",
      " 1.15262041e-05 0.00000000e+00 6.55705846e-04 0.00000000e+00\n",
      " 1.25230373e-04 1.36600790e-03 7.80831365e-03 0.00000000e+00\n",
      " 0.00000000e+00 8.45855072e-03 9.16780501e-02 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "Vs = np.zeros(nstates)\n",
    "PR = np.sum(mdp.Psas * mdp.Rsas, axis=2)\n",
    "gPsas = mdp.gamma * mdp.Psas\n",
    "\n",
    "for i in range(niter):\n",
    "    for s in range(nstates):\n",
    "        Vs[s] = np.max(policy.prob[s, :] *\n",
    "                       (PR[s, :] + np.squeeze(np.dot(gPsas[s, :, :], Vs))))\n",
    "\n",
    "time2 = time.time()\n",
    "print(time2-time1)\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array operations only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14261531829833984\n",
      "[1.55787288e-06 5.61039750e-06 6.01564996e-05 6.01564996e-06\n",
      " 1.15262041e-05 0.00000000e+00 6.55705846e-04 0.00000000e+00\n",
      " 1.25230373e-04 1.36600790e-03 7.80831365e-03 0.00000000e+00\n",
      " 0.00000000e+00 8.45855072e-03 9.16780501e-02 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "nsa = nstates * nactions\n",
    "Vs = np.zeros(nstates)\n",
    "prob = np.reshape(policy.prob, (nsa,), order=\"c\")\n",
    "PR = np.sum(mdp.Psas * mdp.Rsas, axis=2)\n",
    "PR = np.reshape(PR, (nsa, ), order=\"c\")\n",
    "gPsa = mdp.gamma * np.reshape(mdp.Psas, (nsa, nstates), order=\"c\")\n",
    "\n",
    "for i in range(niter):\n",
    "    Q = prob * (PR + np.dot(gPsa, Vs))\n",
    "    Q = np.reshape(Q, (nstates, nactions), order=\"c\")\n",
    "    Vs = np.max(Q, axis=1)\n",
    "\n",
    "time2 = time.time()\n",
    "print(time2-time1)\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
